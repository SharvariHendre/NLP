{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e387ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting nltk\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: click in c:\\users\\dell\\appdata\\roaming\\python\\python313\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\dell\\appdata\\roaming\\python\\python313\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\dell\\appdata\\roaming\\python\\python313\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\dell\\appdata\\roaming\\python\\python313\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\dell\\appdata\\roaming\\python\\python313\\site-packages (from click->nltk) (0.4.6)\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.9.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44587b62",
   "metadata": {},
   "source": [
    "WhiteSpace Based Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fe19111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Text\n",
      "Hello Everyone 0 9 0 9,ðŸ˜ƒ\n",
      "Welcome to Natural Language Processing.\n",
      "\n",
      "Tokenized Text\n",
      "['Hello', 'Everyone', '0', '9', '0', '9,ðŸ˜ƒ', 'Welcome', 'to', 'Natural', 'Language', 'Processing.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "wt = WhitespaceTokenizer()\n",
    "text= \"Hello Everyone 0 9 0 9,ðŸ˜ƒ\\nWelcome to Natural Language Processing.\"\n",
    "# text=\"ðŸ˜ƒ ðŸ˜ƒ ðŸ˜ƒ\"\n",
    "print(\"\\nOriginal Text\")\n",
    "print(text)\n",
    "\n",
    "tokenized_text = wt.tokenize(text)\n",
    "print(\"\\nTokenized Text\")\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1e9eeb",
   "metadata": {},
   "source": [
    "Punctuation Based Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f48e093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Text\n",
      "Hello Everyone 0 0 9. 'WelcomeðŸ˜ƒ to Natural' Language Processing.\n",
      "\n",
      "Tokenized Text\n",
      "['Hello', 'Everyone', '0', '0', '9', '.', \"'\", 'Welcome', 'ðŸ˜ƒ', 'to', 'Natural', \"'\", 'Language', 'Processing', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "text=\"Hello Everyone 0 0 9. 'WelcomeðŸ˜ƒ to Natural' Language Processing.\"\n",
    "# text=\"ðŸ˜ƒ ðŸ˜ƒ ðŸ˜ƒ ðŸ˜ƒ ðŸ˜ƒ\"\n",
    "print(\"\\nOriginal Text\")\n",
    "print(text)\n",
    "result= WordPunctTokenizer().tokenize(text)\n",
    "print(\"\\nTokenized Text\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9041eaf",
   "metadata": {},
   "source": [
    "TreeBank Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea8619d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Everyone', '0', '0', '9', '.', \"'\", 'Welcome', 'ðŸ˜ƒ', 'to', 'Natural', \"'\", 'Language', 'Processing', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer= TreebankWordTokenizer().tokenize(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64d32636",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Everyone',\n",
       " '0',\n",
       " '0',\n",
       " '9.',\n",
       " \"'WelcomeðŸ˜ƒ\",\n",
       " 'to',\n",
       " 'Natural',\n",
       " \"'\",\n",
       " 'Language',\n",
       " 'Processing',\n",
       " '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer= TreebankWordTokenizer()\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b2260b",
   "metadata": {},
   "source": [
    "Tweet Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "918eccc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Everyone', '9', 'ðŸ˜ƒ', ',', 'have', 'a', 'good', 'day', 'ahead', 'ðŸ˜ƒ', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tweet_tokenize = TweetTokenizer()\n",
    "sample_tweet = \"Hello Everyone 9ðŸ˜ƒ, have a good day ahead ðŸ˜ƒ!\"\n",
    "print(tweet_tokenize.tokenize(sample_tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7294d130",
   "metadata": {},
   "source": [
    "MultiWord Expression Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75a9949b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In', 'a', 'little', 'or', 'a_little_bit', 'or', 'a_lot', 'in_spite_of']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import MWETokenizer\n",
    "tokenizer = MWETokenizer([('a', 'little\"'), ('a', 'little', 'bit'), ('a', 'lot')])\n",
    "tokenizer.add_mwe(('in', 'spite', 'of'))\n",
    "tokenizer.tokenize('In a little or a little bit or a lot in spite of'.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6e7e6d",
   "metadata": {},
   "source": [
    "Porters Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4cb0a48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer,LancasterStemmer, RegexpStemmer, SnowballStemmer\n",
    "l= LancasterStemmer()\n",
    "r=RegexpStemmer(\"ing\")\n",
    "p=PorterStemmer()\n",
    "s=SnowballStemmer(\"english\")\n",
    "\n",
    "s.stem(\"running\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ab0e371",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('arabic',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'hungarian',\n",
       " 'italian',\n",
       " 'norwegian',\n",
       " 'porter',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'spanish',\n",
       " 'swedish')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "SnowballStemmer.languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7fcae9cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original words: ['running', 'jumps', 'happily', 'running', 'happilly']\n",
      "Stemmed words: ['run', 'jump', 'happili', 'run', 'happilli']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "words = [\"running\", \"jumps\", \"happily\", \"running\", \"happilly\"]\n",
    "\n",
    "\n",
    "stemmed_words = [porter_stemmer.stem(word) for word in words]\n",
    "\n",
    "print(\"Original words:\", words)\n",
    "print(\"Stemmed words:\", stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aae930f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab3a494f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'walk walk walk walk'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stem_words(text):\n",
    "  return ' '.join([ps.stem(word) for word in text.split()])\n",
    "\n",
    "\n",
    "sample = 'walk walks walking walked'\n",
    "stem_words(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed87adc",
   "metadata": {},
   "source": [
    "Snowball Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6049019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original words: ['running', 'jumped', 'happy', 'happily', 'quickly', 'foxes']\n",
      "Stemmed words: ['run', 'jump', 'happi', 'happili', 'quick', 'fox']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "\n",
    "words_to_stem = ['running', 'jumped', 'happy','happily', 'quickly', 'foxes']\n",
    "\n",
    "stemmed_words = [stemmer.stem(word) for word in words_to_stem]\n",
    "\n",
    "print(\"Original words:\", words_to_stem)\n",
    "print(\"Stemmed words:\", stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2afd0b8",
   "metadata": {},
   "source": [
    "Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2db9af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ebb1f92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed2c6882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mouse'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wl.lemmatize(\"mice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "624ad572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kites ---> kite\n",
      "babies ---> baby\n",
      "dogs ---> dog\n",
      "flying ---> flying\n",
      "smiling ---> smiling\n",
      "driving ---> driving\n",
      "died ---> died\n",
      "tried ---> tried\n",
      "feet ---> foot\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "list1 = ['kites', 'babies', 'dogs', 'flying', 'smiling',\n",
    "         'driving', 'died', 'tried', 'feet']\n",
    "for words in list1:\n",
    "    print(words + \" ---> \" + wnl.lemmatize(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc07226",
   "metadata": {},
   "source": [
    "Sentence Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7beaead0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'cat', 'is', 'sitting', 'with', 'the', 'bats', 'on', 'the', 'striped', 'mat', 'under', 'many', 'flying', 'geese']\n",
      "the cat is sitting with the bat on the striped mat under many flying goose\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "string = 'the cat is sitting with the bats on the striped mat under many flying geese'\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "# Converting String into tokens\n",
    "list2 = nltk.word_tokenize(string)\n",
    "print(list2)\n",
    "#> ['the', 'cat', 'is', 'sitting', 'with', 'the', 'bats', 'on',\n",
    "# 'the', 'striped', 'mat', 'under', 'many', 'flying', 'geese']\n",
    "\n",
    "lemmatized_string = ' '.join([wnl.lemmatize(words) for words in list2])\n",
    "\n",
    "print(lemmatized_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf756b6",
   "metadata": {},
   "source": [
    "After Stopwords Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5ca3b7ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7a9efba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat ---> cat\n",
      "sitting ---> sitting\n",
      "bats ---> bat\n",
      "striped ---> striped\n",
      "mat ---> mat\n",
      "many ---> many\n",
      "flying ---> flying\n",
      "geese ---> goose\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "for words in list2:\n",
    "  if words not in stopwords.words('english'):\n",
    "    print(words + \" ---> \" + wnl.lemmatize(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f195d90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
